{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xJckl99IHePQ"
      },
      "source": [
        "## all-MiniLM-L6-v1 Hugging Face Model to ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IWvxTyMSHnFo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q transformers transformers[onnx] transformers[sentencepiece] einops\n",
        "%pip install -q torch torchaudio \n",
        "%pip install -U -q sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v1  models/all-MiniLM-L6-v1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converter Help  \n",
        "\n",
        "```\n",
        "usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL\n",
        "                                               [--feature FEATURE]\n",
        "                                               [--opset OPSET] [--atol ATOL]\n",
        "                                               [--framework {pt,tf}]\n",
        "                                               [--cache_dir CACHE_DIR]\n",
        "                                               [--preprocessor {auto,tokenizer,feature_extractor,processor}]\n",
        "                                               [--export_with_transformers]\n",
        "                                               output\n",
        "\n",
        "positional arguments:\n",
        "  output                Path indicating where to store generated ONNX model.\n",
        "\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "  -m MODEL, --model MODEL\n",
        "                        Model ID on huggingface.co or path on disk to load\n",
        "                        model from.\n",
        "  --feature FEATURE     The type of features to export the model with.\n",
        "  --opset OPSET         ONNX opset version to export the model with.\n",
        "  --atol ATOL           Absolute difference tolerance when validating the\n",
        "                        model.\n",
        "  --framework {pt,tf}   The framework to use for the ONNX export. If not\n",
        "                        provided, will attempt to use the local checkpoint's\n",
        "                        original framework or what is available in the\n",
        "                        environment.\n",
        "  --cache_dir CACHE_DIR\n",
        "                        Path indicating where to store cache.\n",
        "  --preprocessor {auto,tokenizer,feature_extractor,processor}\n",
        "                        Which type of preprocessor to use. 'auto' tries to\n",
        "                        automatically detect it.\n",
        "  --export_with_transformers\n",
        "                        Whether to use transformers.onnx instead of\n",
        "                        optimum.exporters.onnx to perform the ONNX export. It\n",
        "                        can be useful when exporting a model supported in\n",
        "                        transformers but not in optimum, otherwise it is not\n",
        "                        recommended.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.2/136.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (1.24.3)\n",
            "Requirement already satisfied: coloredlogs in ./venv/lib/python3.10/site-packages (from onnxruntime-gpu) (15.0.1)\n",
            "Requirement already satisfied: protobuf in ./venv/lib/python3.10/site-packages (from onnxruntime-gpu) (3.20.2)\n",
            "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from onnxruntime-gpu) (1.12)\n",
            "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from onnxruntime-gpu) (23.1)\n",
            "Requirement already satisfied: flatbuffers in ./venv/lib/python3.10/site-packages (from onnxruntime-gpu) (2.0.7)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.10/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Installing collected packages: onnxruntime-gpu\n",
            "Successfully installed onnxruntime-gpu-1.14.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install onnxruntime-gpu numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "on_model = \"mpt_onnx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-23 16:00:40.380510020 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.31/attn/Trilu'\n",
            "2023-05-23 16:00:40.382602265 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.30/attn/Trilu'\n",
            "2023-05-23 16:00:40.384902956 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.29/attn/Trilu'\n",
            "2023-05-23 16:00:40.387151006 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.28/attn/Trilu'\n",
            "2023-05-23 16:00:40.389247712 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.27/attn/Trilu'\n",
            "2023-05-23 16:00:40.391449754 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.26/attn/Trilu'\n",
            "2023-05-23 16:00:40.393672945 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.25/attn/Trilu'\n",
            "2023-05-23 16:00:40.395924361 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.24/attn/Trilu'\n",
            "2023-05-23 16:00:40.397926375 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.23/attn/Trilu'\n",
            "2023-05-23 16:00:40.400104693 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.22/attn/Trilu'\n",
            "2023-05-23 16:00:40.402170382 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.21/attn/Trilu'\n",
            "2023-05-23 16:00:40.404365132 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.20/attn/Trilu'\n",
            "2023-05-23 16:00:40.406528695 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.19/attn/Trilu'\n",
            "2023-05-23 16:00:40.408718804 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.18/attn/Trilu'\n",
            "2023-05-23 16:00:40.410916506 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.17/attn/Trilu'\n",
            "2023-05-23 16:00:40.413108066 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.16/attn/Trilu'\n",
            "2023-05-23 16:00:40.415281516 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.15/attn/Trilu'\n",
            "2023-05-23 16:00:40.417349785 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.14/attn/Trilu'\n",
            "2023-05-23 16:00:40.419595763 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.13/attn/Trilu'\n",
            "2023-05-23 16:00:40.421788326 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.12/attn/Trilu'\n",
            "2023-05-23 16:00:40.423855089 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.11/attn/Trilu'\n",
            "2023-05-23 16:00:40.426098945 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.10/attn/Trilu'\n",
            "2023-05-23 16:00:40.428290557 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.9/attn/Trilu'\n",
            "2023-05-23 16:00:40.430469583 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.8/attn/Trilu'\n",
            "2023-05-23 16:00:40.432638691 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.7/attn/Trilu'\n",
            "2023-05-23 16:00:40.434778395 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.6/attn/Trilu'\n",
            "2023-05-23 16:00:40.436939016 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.5/attn/Trilu'\n",
            "2023-05-23 16:00:40.439031228 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.4/attn/Trilu'\n",
            "2023-05-23 16:00:40.441230247 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.3/attn/Trilu'\n",
            "2023-05-23 16:00:40.443374397 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.2/attn/Trilu'\n",
            "2023-05-23 16:00:40.445473814 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.1/attn/Trilu'\n",
            "2023-05-23 16:00:40.447597616 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.0/attn/Trilu'\n",
            "2023-05-23 16:00:40.498906984 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.31/attn/Trilu'\n",
            "2023-05-23 16:00:40.500986897 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.30/attn/Trilu'\n",
            "2023-05-23 16:00:40.503216274 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.29/attn/Trilu'\n",
            "2023-05-23 16:00:40.505438138 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.28/attn/Trilu'\n",
            "2023-05-23 16:00:40.507627162 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.27/attn/Trilu'\n",
            "2023-05-23 16:00:40.509935468 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.26/attn/Trilu'\n",
            "2023-05-23 16:00:40.512183950 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.25/attn/Trilu'\n",
            "2023-05-23 16:00:40.514379764 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.24/attn/Trilu'\n",
            "2023-05-23 16:00:40.516481683 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.23/attn/Trilu'\n",
            "2023-05-23 16:00:40.518738734 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.22/attn/Trilu'\n",
            "2023-05-23 16:00:40.520852973 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.21/attn/Trilu'\n",
            "2023-05-23 16:00:40.522993664 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.20/attn/Trilu'\n",
            "2023-05-23 16:00:40.525339965 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.19/attn/Trilu'\n",
            "2023-05-23 16:00:40.527588680 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.18/attn/Trilu'\n",
            "2023-05-23 16:00:40.529827975 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.17/attn/Trilu'\n",
            "2023-05-23 16:00:40.532039427 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.16/attn/Trilu'\n",
            "2023-05-23 16:00:40.534254204 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.15/attn/Trilu'\n",
            "2023-05-23 16:00:40.536395423 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.14/attn/Trilu'\n",
            "2023-05-23 16:00:40.538595289 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.13/attn/Trilu'\n",
            "2023-05-23 16:00:40.541000505 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.12/attn/Trilu'\n",
            "2023-05-23 16:00:40.543219048 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.11/attn/Trilu'\n",
            "2023-05-23 16:00:40.545476318 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.10/attn/Trilu'\n",
            "2023-05-23 16:00:40.547813325 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a "
          ]
        },
        {
          "ename": "RuntimeException",
          "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:368 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to allocate memory for requested buffer of size 201326592\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnxruntime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnxrun\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(nxrun\u001b[39m.\u001b[39mget_available_providers())\n\u001b[0;32m----> 3\u001b[0m sess \u001b[39m=\u001b[39m nxrun\u001b[39m.\u001b[39;49mInferenceSession(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mraw-files/\u001b[39;49m\u001b[39m{\u001b[39;49;00mon_model\u001b[39m}\u001b[39;49;00m\u001b[39m/model.onnx\u001b[39;49m\u001b[39m'\u001b[39;49m, providers\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mCUDAExecutionProvider\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
            "File \u001b[0;32m/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:360\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m disabled_optimizers \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisabled_optimizers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    361\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
            "File \u001b[0;32m/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:408\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    405\u001b[0m     disabled_optimizers \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    407\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    410\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n\u001b[1;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess\u001b[39m.\u001b[39msession_options\n",
            "\u001b[0;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:368 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to allocate memory for requested buffer of size 201326592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.9/attn/Trilu'\n",
            "2023-05-23 16:00:40.549910479 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.8/attn/Trilu'\n",
            "2023-05-23 16:00:40.552056950 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.7/attn/Trilu'\n",
            "2023-05-23 16:00:40.554259745 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.6/attn/Trilu'\n",
            "2023-05-23 16:00:40.556422240 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.5/attn/Trilu'\n",
            "2023-05-23 16:00:40.558444636 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.4/attn/Trilu'\n",
            "2023-05-23 16:00:40.560605788 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.3/attn/Trilu'\n",
            "2023-05-23 16:00:40.562728559 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.2/attn/Trilu'\n",
            "2023-05-23 16:00:40.564728600 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.1/attn/Trilu'\n",
            "2023-05-23 16:00:40.566945388 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.0/attn/Trilu'\n",
            "2023-05-23 16:00:40.585842223 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.31/attn/Trilu'\n",
            "2023-05-23 16:00:40.587937998 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.30/attn/Trilu'\n",
            "2023-05-23 16:00:40.590142885 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.29/attn/Trilu'\n",
            "2023-05-23 16:00:40.592319870 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.28/attn/Trilu'\n",
            "2023-05-23 16:00:40.594409335 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.27/attn/Trilu'\n",
            "2023-05-23 16:00:40.596683594 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.26/attn/Trilu'\n",
            "2023-05-23 16:00:40.598895899 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.25/attn/Trilu'\n",
            "2023-05-23 16:00:40.601037782 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.24/attn/Trilu'\n",
            "2023-05-23 16:00:40.603103915 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.23/attn/Trilu'\n",
            "2023-05-23 16:00:40.605343848 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.22/attn/Trilu'\n",
            "2023-05-23 16:00:40.607461130 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.21/attn/Trilu'\n",
            "2023-05-23 16:00:40.609693069 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.20/attn/Trilu'\n",
            "2023-05-23 16:00:40.611910681 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.19/attn/Trilu'\n",
            "2023-05-23 16:00:40.614140019 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.18/attn/Trilu'\n",
            "2023-05-23 16:00:40.616332352 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.17/attn/Trilu'\n",
            "2023-05-23 16:00:40.618548842 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.16/attn/Trilu'\n",
            "2023-05-23 16:00:40.620821014 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.15/attn/Trilu'\n",
            "2023-05-23 16:00:40.622914356 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.14/attn/Trilu'\n",
            "2023-05-23 16:00:40.625135137 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.13/attn/Trilu'\n",
            "2023-05-23 16:00:40.627275466 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.12/attn/Trilu'\n",
            "2023-05-23 16:00:40.629310203 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.11/attn/Trilu'\n",
            "2023-05-23 16:00:40.631650555 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.10/attn/Trilu'\n",
            "2023-05-23 16:00:40.633817275 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.9/attn/Trilu'\n",
            "2023-05-23 16:00:40.635896670 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.8/attn/Trilu'\n",
            "2023-05-23 16:00:40.638058071 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.7/attn/Trilu'\n",
            "2023-05-23 16:00:40.640257371 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.6/attn/Trilu'\n",
            "2023-05-23 16:00:40.642455721 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.5/attn/Trilu'\n",
            "2023-05-23 16:00:40.644525459 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.4/attn/Trilu'\n",
            "2023-05-23 16:00:40.646706724 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.3/attn/Trilu'\n",
            "2023-05-23 16:00:40.648987868 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.2/attn/Trilu'\n",
            "2023-05-23 16:00:40.651007380 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.1/attn/Trilu'\n",
            "2023-05-23 16:00:40.653171126 [W:onnxruntime:, constant_folding.cc:179 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Trilu node '/transformer/blocks.0/attn/Trilu'\n",
            "2023-05-23 16:00:40.838448220 [E:onnxruntime:, inference_session.cc:1532 operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:368 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to allocate memory for requested buffer of size 201326592\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime as nxrun\n",
        "print(nxrun.get_available_providers())\n",
        "sess = nxrun.InferenceSession(f'raw-files/{on_model}/model.onnx', providers=[\"CUDAExecutionProvider\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = 'def fibonacci(n):'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"write a python funciton which takes a number, add and multiply with 100 and returns<|endoftext|>\"\n",
        "input_text = \"python function that reads a file, and shows its content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m input_ids\u001b[39m.\u001b[39mextend([\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m2048\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(input_ids)))\n\u001b[1;32m      9\u001b[0m attention_mask\u001b[39m.\u001b[39mextend([\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m2048\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(attention_mask)))\n\u001b[0;32m---> 11\u001b[0m input_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(input_ids, np\u001b[39m.\u001b[39mint64)\n\u001b[1;32m     12\u001b[0m attention_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(attention_mask, np\u001b[39m.\u001b[39mbool_)\n\u001b[1;32m     13\u001b[0m \u001b[39m# input_ids.shape, attention_mask\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('models/mpt-7b-chat', trust_remote_code=True)\n",
        "input_ids = tokenizer(input_text)[\"input_ids\"]\n",
        "attention_mask = tokenizer(input_text)[\"attention_mask\"]\n",
        "mask = len(attention_mask)\n",
        "print(len(input_ids))\n",
        "input_ids.extend([0] * (2048 - len(input_ids)))\n",
        "attention_mask.extend([0] * (2048 - len(attention_mask)))\n",
        "\n",
        "input_ids = np.asarray(input_ids, np.int64)\n",
        "attention_mask = np.asarray(attention_mask, np.bool_)\n",
        "# input_ids.shape, attention_mask\n",
        "attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_name = sess.get_inputs()[0].name\n",
        "input_name2 = sess.get_inputs()[1].name\n",
        "input_shape = sess.get_inputs()[0].shape\n",
        "input_shape2 = sess.get_inputs()[1].shape\n",
        "output_name = sess.get_outputs()[0].name\n",
        "output_shape = sess.get_outputs()[0].shape\n",
        "# run onnx model with onnx runtime python\n",
        "# result = sess.run(None, {input_name: np.ones((1, 2048), dtype=np.int64), input_name2: np.ones((1, 2048), dtype=np.bool_)})\n",
        "result = sess.run(None, {input_name: np.expand_dims(input_ids, 0), input_name2: np.expand_dims(attention_mask, 0)})\n",
        "\n",
        "# print(\"Output\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = list()\n",
        "for idx, i in enumerate(result[0][0]):\n",
        "    top = int(np.argmax(i))\n",
        "    t.append(top)\n",
        "    # if idx == (mask - 1):\n",
        "    #     break\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'- to takes a text and converts returns the contents. on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_code = tokenizer.decode(t, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "generated_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1, 2048, 50432)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.asarray(result).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWB2G_kLHou2",
        "outputId": "b66c9ef4-8536-40de-cf47-fe2aeb9410c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local PyTorch model found.\n",
            "Framework not requested. Using torch to export to ONNX.\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at models/all-MiniLM-L6-v1 and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using framework PyTorch: 2.0.1+cpu\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> False\n",
            "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "Validating ONNX model...\n",
            "\t-[✓] ONNX model output names match reference model ({'logits'})\n",
            "\t- Validating ONNX Model output \"logits\":\n",
            "\t\t-[✓] (3, 9, 30522) matches (3, 9, 30522)\n",
            "\t\t-[✓] all values close (atol: 0.005)\n",
            "All good, model saved at: raw-files/minilm_onnx/model.onnx\n",
            "/media/ahwar/WD_Swap/all-MiniLM-L6-v1/venv/lib/python3.10/site-packages/transformers/onnx/__main__.py:178: FutureWarning: The export was done by transformers.onnx which is deprecated and will be removed in v5. We recommend using optimum.exporters.onnx in future. You can find more information here: https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!python -m transformers.onnx --opset 16 --atol 0.005 \\\n",
        "    --feature=causal-lm --export_with_transformers \\\n",
        "    --model=models/all-MiniLM-L6-v1 raw-files/minilm_onnx/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MPTForCausalLM(\n",
              "  (transformer): MPTModel(\n",
              "    (wte): Embedding(50432, 4096)\n",
              "    (emb_drop): Dropout(p=0, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0-31): 32 x MPTBlock(\n",
              "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiheadAttention(\n",
              "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn): MPTMLP(\n",
              "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
              "          (act): GELU(approximate='none')\n",
              "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
              "        )\n",
              "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
              "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahwar/.cache/huggingface/modules/transformers_modules/mpt-7b-chat/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
            "  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# load model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mmodels/mpt-7b-chat\u001b[39;49m\u001b[39m'\u001b[39;49m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mmodels/mpt-7b-chat\u001b[39m\u001b[39m'\u001b[39m, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m x \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(input_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m320\u001b[39m)\n",
            "File \u001b[0;32m/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:462\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m     class_ref \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mauto_map[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\n\u001b[1;32m    459\u001b[0m     model_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    460\u001b[0m         class_ref, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    461\u001b[0m     )\n\u001b[0;32m--> 462\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    463\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    464\u001b[0m     )\n\u001b[1;32m    465\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    466\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
            "File \u001b[0;32m/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2611\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2608\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2610\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2611\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2613\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2614\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mpt-7b-chat/modeling_mpt.py:205\u001b[0m, in \u001b[0;36mMPTForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mtie_word_embeddings:\n\u001b[1;32m    204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mMPTForCausalLM only supports tied word embeddings\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m MPTModel(config)\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogit_scale \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mlogit_scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mpt-7b-chat/modeling_mpt.py:46\u001b[0m, in \u001b[0;36mMPTModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_seq_len, config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_drop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39memb_pdrop)\n\u001b[0;32m---> 46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([MPTBlock(device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig\u001b[39m.\u001b[39mto_dict()) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layers)])\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_f \u001b[39m=\u001b[39m norm_class(config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39minit_device \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mpt-7b-chat/modeling_mpt.py:46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_seq_len, config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_drop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39memb_pdrop)\n\u001b[0;32m---> 46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([MPTBlock(device\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49minit_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig\u001b[39m.\u001b[39;49mto_dict()) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layers)])\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_f \u001b[39m=\u001b[39m norm_class(config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39minit_device \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mpt-7b-chat/blocks.py:28\u001b[0m, in \u001b[0;36mMPTBlock.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     26\u001b[0m attn_class \u001b[39m=\u001b[39m ATTN_CLASS_REGISTRY[attn_config[\u001b[39m'\u001b[39m\u001b[39mattn_type\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_1 \u001b[39m=\u001b[39m norm_class(d_model, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m attn_class(attn_impl\u001b[39m=\u001b[39;49mattn_config[\u001b[39m'\u001b[39;49m\u001b[39mattn_impl\u001b[39;49m\u001b[39m'\u001b[39;49m], clip_qkv\u001b[39m=\u001b[39;49mattn_config[\u001b[39m'\u001b[39;49m\u001b[39mclip_qkv\u001b[39;49m\u001b[39m'\u001b[39;49m], qk_ln\u001b[39m=\u001b[39;49mattn_config[\u001b[39m'\u001b[39;49m\u001b[39mqk_ln\u001b[39;49m\u001b[39m'\u001b[39;49m], softmax_scale\u001b[39m=\u001b[39;49mattn_config[\u001b[39m'\u001b[39;49m\u001b[39msoftmax_scale\u001b[39;49m\u001b[39m'\u001b[39;49m], attn_pdrop\u001b[39m=\u001b[39;49mattn_config[\u001b[39m'\u001b[39;49m\u001b[39mattn_pdrop\u001b[39;49m\u001b[39m'\u001b[39;49m], d_model\u001b[39m=\u001b[39;49md_model, n_heads\u001b[39m=\u001b[39;49mn_heads, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_2 \u001b[39m=\u001b[39m norm_class(d_model, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn \u001b[39m=\u001b[39m MPTMLP(d_model\u001b[39m=\u001b[39md_model, expansion_ratio\u001b[39m=\u001b[39mexpansion_ratio, device\u001b[39m=\u001b[39mdevice)\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/mpt-7b-chat/attention.py:151\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, d_model, n_heads, attn_impl, clip_qkv, qk_ln, softmax_scale, attn_pdrop, low_precision_layernorm, device)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mattn_impl=\u001b[39m\u001b[39m{\u001b[39;00mattn_impl\u001b[39m!r}\u001b[39;00m\u001b[39m is an invalid setting.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md_model, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj\u001b[39m.\u001b[39m_is_residual \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
            "File \u001b[0;32m/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
            "File \u001b[0;32m/media/ahwar/WD_Swap/mpt-7b-chat/venv/lib/python3.10/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "# load model\n",
        "model = AutoModelForCausalLM.from_pretrained('models/mpt-7b-chat', trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained('models/mpt-7b-chat', trust_remote_code=True)\n",
        "x = tokenizer.encode(input_text, return_tensors='pt', max_length=320)\n",
        "# x = tokenizer(input_text, return_tensors=\"pt\")\n",
        "# x[\"attention_mask\"] = x[\"attention_mask\"].type(torch.bool)\n",
        "# x\n",
        "# tokenizer\n",
        "# tokenizer.encode\n",
        "model.to(device='cpu:0')\n",
        "model.type(torch.bfloat16)\n",
        "\n",
        "\n",
        "# # single input encoding + generation\n",
        "x = tokenizer.encode(input_text, return_tensors='pt', max_length=320)\n",
        "y = model.generate(x)\n",
        "\n",
        "# decoding, clean_up_tokenization_spaces=False to ensure syntactical correctness\n",
        "generated_code = tokenizer.decode(y[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(generated_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "# load model\n",
        "model = AutoModelForCausalLM.from_pretrained('models/mpt-7b-chat', trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained('models/mpt-7b-chat', trust_remote_code=True)\n",
        "# tokenizer\n",
        "# tokenizer.encode\n",
        "model.to(device='cpu:0')\n",
        "model.type(torch.bfloat16)\n",
        "\n",
        "\n",
        "# # single input encoding + generation\n",
        "x = tokenizer.encode(input_text, return_tensors='pt', max_length=320)\n",
        "y = model.generate(x)\n",
        "\n",
        "# decoding, clean_up_tokenization_spaces=False to ensure syntactical correctness\n",
        "generated_code = tokenizer.decode(y[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(generated_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([16659,  1159,   326,  9563,   247,  1873,    13,   285,  2722,   697,\n",
              "         2600,    15,   187,   187, 11202, 16659,   187,  1545,  1239,    64])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1545  5713   251 42401     9    79  2262     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 1545,  5713,   251, 42401,     9,    79,  2262,     0]])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(input_ids)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1552, 310, 247, 2159, 3425, 15]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence_a = \"This is a short sequence.\"\n",
        "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
        "\n",
        "encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\n",
        "encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]\n",
        "encoded_sequence_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8])\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)\n",
        "y.shape\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "437c6a4c2ab8ad564298253974ee7794b68bcbeea462aed8eaaa05b6d7c57f73"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
